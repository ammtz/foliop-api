import pandas as pd
import yfinance as yf
import pickle
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import os
import logging

class DataManager:
    def __init__(self, 
                 baseline_path: str = "data/baseline_sp500.pkl",
                 updated_path: str = "data/updated_sp500.pkl",
                 sp500_tickers_path: str = "data/sp500_tickers.csv"):
        """
        Initialize the DataManager with file paths.
        """
        self.baseline_path = baseline_path
        self.updated_path = updated_path
        self.sp500_tickers_path = sp500_tickers_path

        # Ensure data directory exists
        os.makedirs(os.path.dirname(baseline_path), exist_ok=True)

        # Setup logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)

    def update_sp500_tickers(self) -> None:
        """
        Fetch the latest S&P 500 tickers from Wikipedia and update the CSV file.
        """
        try:
            self.logger.info("Fetching latest S&P 500 tickers...")

            # Wikipedia URL for S&P 500 companies
            url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"

            # Fetch the webpage
            response = requests.get(url)
            soup = BeautifulSoup(response.text, "html.parser")

            # Find the table containing the S&P 500 list
            table = soup.find("table", {"id": "constituents"})
            if table is None:
                raise ValueError("Failed to find S&P 500 table on Wikipedia")

            # Extract rows from the table
            rows = table.find_all("tr")[1:]  # Skip header row

            # Initialize lists for tickers and company names
            companies, tickers = [], []

            # Extract ticker symbols and company names
            for row in rows:
                cols = row.find_all("td")
                if len(cols) >= 2:
                    tickers.append(cols[0].text.strip())  # Ticker symbol
                    companies.append(cols[1].text.strip())  # Company name

            # Create DataFrame and save as CSV
            sp500 = pd.DataFrame({"Company": companies, "Ticker": tickers})
            sp500.to_csv(self.sp500_tickers_path, index=False)

            self.logger.info("S&P 500 tickers updated successfully")

        except Exception as e:
            self.logger.error(f"Error updating S&P 500 tickers: {str(e)}")
            raise

    def update_dataset(self) -> None:
        """
        Update the dataset with the latest stock data from yfinance.
        """
        try:
            self.logger.info("Updating dataset...")

            # First, update S&P 500 tickers
            self.update_sp500_tickers()

            # Load baseline data if updated dataset doesn't exist
            if not os.path.exists(self.updated_path) and os.path.exists(self.baseline_path):
                with open(self.baseline_path, "rb") as f:
                    dataset = pickle.load(f)
                self.logger.info("Loaded baseline dataset as starting point")
            elif os.path.exists(self.updated_path):
                with open(self.updated_path, "rb") as f:
                    dataset = pickle.load(f)
                self.logger.info("Loaded existing updated dataset")
            else:
                raise FileNotFoundError("Neither baseline nor updated dataset exists")

            # Get last date in the dataset
            last_date = dataset["historical_data"].index[-1]

            # Calculate start date for update (last date + 1 day)
            start_date = last_date + timedelta(days=1)
            end_date = datetime.now()

            # Skip update if already up to date
            if start_date.date() >= end_date.date():
                self.logger.info("Dataset is already up to date")
                return

            # Load latest tickers from updated CSV
            sp500 = pd.read_csv(self.sp500_tickers_path)
            tickers = sp500["Ticker"].tolist()

            # Download new data
            self.logger.info(f"Downloading new data from {start_date.date()} to {end_date.date()}")
            new_data = yf.download(tickers, start=start_date, end=end_date)["Adj Close"]

            # Combine datasets
            updated_data = pd.concat([dataset["historical_data"], new_data])

            # Update dataset dictionary
            dataset.update({
                "historical_data": updated_data,
                "last_updated": datetime.now(),
                "date_range": {
                    "start": dataset["date_range"]["start"],
                    "end": end_date.strftime("%Y-%m-%d")
                }
            })

            # Save updated dataset
            with open(self.updated_path, "wb") as f:
                pickle.dump(dataset, f)

            self.logger.info("Dataset updated successfully")

        except Exception as e:
            self.logger.error(f"Error updating dataset: {str(e)}")
            raise

    def get_dataset(self, use_updated: bool = True) -> dict:
        """
        Load and return the specified dataset.
        """
        try:
            file_path = self.updated_path if use_updated else self.baseline_path

            if not os.path.exists(file_path):
                raise FileNotFoundError(f"Dataset not found at {file_path}")

            with open(file_path, "rb") as f:
                dataset = pickle.load(f)

            return dataset

        except Exception as e:
            self.logger.error(f"Error loading dataset: {str(e)}")
            raise

    def get_data_info(self) -> dict:
        """
        Get information about both datasets.
        """
        info = {}

        for dataset_type in ["baseline", "updated"]:
            path = self.baseline_path if dataset_type == "baseline" else self.updated_path

            try:
                if os.path.exists(path):
                    with open(path, "rb") as f:
                        dataset = pickle.load(f)

                    info[dataset_type] = {
                        "number_of_tickers": len(dataset["historical_data"].columns),
                        "date_range": dataset["date_range"],
                        "last_updated": dataset["last_updated"],
                        "file_size_mb": os.path.getsize(path) / (1024 * 1024)
                    }
                else:
                    info[dataset_type] = None

            except Exception as e:
                info[dataset_type] = f"Error: {str(e)}"

        return info

if __name__ == "__main__":
    # Example usage
    data_manager = DataManager()

    # Update dataset (this also updates S&P 500 tickers automatically)
    data_manager.update_dataset()

    # Print dataset info
    print(data_manager.get_data_info())
